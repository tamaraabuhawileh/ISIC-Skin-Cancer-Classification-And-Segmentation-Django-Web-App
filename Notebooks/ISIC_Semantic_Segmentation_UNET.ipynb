{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwE6ldmPhwIe"
      },
      "source": [
        "## Semantic Segmentation : U-Net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oaklQUTiAIp"
      },
      "source": [
        "# Imports and Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsayszxQnYtH"
      },
      "outputs": [],
      "source": [
        "H = 256\n",
        "W = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiCLrE3-nC3J"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "import cv2\n",
        "from glob import glob\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import Recall, Precision\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfeUk7dQiIEQ"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JemOHJzkXF_x",
        "outputId": "58494d29-5a14-4c70-b996-6b170332d3a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WYTB5iti61r"
      },
      "outputs": [],
      "source": [
        "# imgs path \"/content/drive/MyDrive/ISIC2018_Task1-2_Training_Input/ISIC2018_Task1-2_Training_Input\"\n",
        "# masks path \"/content/drive/MyDrive/ISIC2018_Task1_Training_GroundTruth\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6YpVpH1p5LC"
      },
      "source": [
        "# Model Building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmPbulTvmwNm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGPzcBCbqeN4"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdFSYyF2sPUW"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PncaZPqqatq"
      },
      "outputs": [],
      "source": [
        "def conv_block(inputs, num_filters):\n",
        "    x = Conv2D(num_filters, 3, padding=\"same\")(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def encoder_block(inputs, num_filters):\n",
        "    x = conv_block(inputs, num_filters)\n",
        "    p = MaxPool2D((2, 2))(x)\n",
        "    return x, p\n",
        "\n",
        "def decoder_block(inputs, skip_features, num_filters):\n",
        "    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(inputs)\n",
        "    x = Concatenate()([x, skip_features])\n",
        "    x = conv_block(x, num_filters)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9O_bKw8p-Bz"
      },
      "outputs": [],
      "source": [
        "inputs = Input((256, 256, 3))\n",
        "\n",
        "\"\"\" Encoder \"\"\"\n",
        "s1, p1 = encoder_block(inputs, 64)\n",
        "s2, p2 = encoder_block(p1, 128)\n",
        "s3, p3 = encoder_block(p2, 256)\n",
        "s4, p4 =encoder_block(p3, 400)\n",
        "s5, p5 =encoder_block(p4, 800)\n",
        "s6, p6=encoder_block(p5,1400)\n",
        "\n",
        "\"\"\" Bridge \"\"\"\n",
        "b1 = conv_block(p6, 1024)\n",
        "\n",
        "\"\"\" Decoder \"\"\"\n",
        "d1 = decoder_block(b1, s6, 1400)\n",
        "d2 = decoder_block(d1, s5, 800)\n",
        "d3 = decoder_block(d2, s4, 400)\n",
        "d4 = decoder_block(d3, s3, 256)\n",
        "d5 = decoder_block(d4, s2, 128)\n",
        "d6 = decoder_block(d5, s1, 64)\n",
        "\n",
        "\"\"\" Outputs \"\"\"\n",
        "outputs = Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(d6)\n",
        "\n",
        "\"\"\" Model \"\"\"\n",
        "model = Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoTH_sxJnyw4"
      },
      "outputs": [],
      "source": [
        "def iou(y_true, y_pred):\n",
        "    def f(y_true, y_pred):\n",
        "        intersection = (y_true * y_pred).sum()\n",
        "        union = y_true.sum() + y_pred.sum() - intersection\n",
        "        x = (intersection + 1e-15) / (union + 1e-15)\n",
        "        x = x.astype(np.float32)\n",
        "        return x\n",
        "    return tf.numpy_function(f, [y_true, y_pred], tf.float32)\n",
        "\n",
        "smooth = 1e-15\n",
        "\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true = tf.keras.layers.Flatten()(y_true)\n",
        "    y_pred = tf.keras.layers.Flatten()(y_pred)\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    return (2. * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    return 1.0 - dice_coef(y_true, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_IpAoVeiXnNa"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.metrics import Accuracy,Recall,Precision\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "metrics = [dice_coef, iou, Recall(), Precision()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJsOqp6VAxZD"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=\"binary_crossentropy\", optimizer=Adam(0.0001), metrics=metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpzqQaIutFgA"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g49criYcloN0"
      },
      "source": [
        "# Data Preperation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tv8bqBb46aLd"
      },
      "outputs": [],
      "source": [
        "def create_dir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "def shuffling(x, y):\n",
        "    x, y = shuffle(x, y, random_state=42)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "def load_data(dataset_path, split=0.2):\n",
        "    images = sorted(glob(os.path.join(dataset_path, \"ISIC2018_Task1-2_Training_Input/ISIC2018_Task1-2_Training_Input\", \"*.jpg\")))\n",
        "    masks = sorted(glob(os.path.join(dataset_path, \"ISIC2018_Task1_Training_GroundTruth\", \"*.png\")))\n",
        "\n",
        "    test_size = int(len(images) * split)\n",
        "\n",
        "    train_x, valid_x = train_test_split(images, test_size=test_size, random_state=42)\n",
        "    train_y, valid_y = train_test_split(masks, test_size=test_size, random_state=42)\n",
        "\n",
        "    train_x, test_x = train_test_split(train_x, test_size=test_size, random_state=42)\n",
        "    train_y, test_y = train_test_split(train_y, test_size=test_size, random_state=42)\n",
        "\n",
        "    return (train_x, train_y), (valid_x, valid_y), (test_x, test_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6IRZhwc6SL4"
      },
      "outputs": [],
      "source": [
        "def read_image(path):\n",
        "    path = path.decode()  #  converts the path argument from a bytes object to a string\n",
        "    x = cv2.imread(path, cv2.IMREAD_COLOR)  ## (H, W, 3) #. The cv2.IMREAD_COLOR flag indicates that the image should be loaded in the RGB color format, and not as grayscale or with alpha channel information\n",
        "    x = cv2.resize(x, (W, H))\n",
        "    x = x/255.0\n",
        "    x = x.astype(np.float32)\n",
        "    return x                                ## (256, 256, 3)\n",
        "\n",
        "def read_mask(path):\n",
        "    path = path.decode()\n",
        "    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)  ## (H, W)\n",
        "    x = cv2.resize(x, (W, H))\n",
        "    x = x/255.0\n",
        "    x = x.astype(np.float32)                    ## (256, 256)\n",
        "    x = np.expand_dims(x, axis=-1)              ## (256, 256, 1)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ombp0XYj2t2G",
        "outputId": "67e68192-1187-4758-eb17-27403bc8033d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 1558 - 1558\n",
            "Valid: 518 - 518\n",
            "Test: 518 - 518\n"
          ]
        }
      ],
      "source": [
        "def tf_parse(x, y):\n",
        "    def _parse(x, y):\n",
        "        x = read_image(x)\n",
        "        y = read_mask(y)\n",
        "        return x, y\n",
        "\n",
        "    x, y = tf.numpy_function(_parse, [x, y], [tf.float32, tf.float32])\n",
        "    x.set_shape([H, W, 3])\n",
        "    y.set_shape([H, W, 1])\n",
        "    return x, y\n",
        "\n",
        "def tf_dataset(X, Y, batch):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
        "    dataset = dataset.map(tf_parse)\n",
        "    dataset = dataset.batch(batch)\n",
        "    dataset = dataset.prefetch(10)dimension (256,256,3)\n",
        "    return dataset\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\" Seeding \"\"\"\n",
        "    np.random.seed(42)\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    \"\"\" Folder for saving data \"\"\"\n",
        "    create_dir(\"/content/drive/MyDrive/UNET\")\n",
        "\n",
        "    \"\"\" Hyperparameters \"\"\"\n",
        "    batch_size = 20\n",
        "    lr = 1e-2 ## (0.0001)\n",
        "    num_epoch = 10\n",
        "    model_path = \"/content/drive/MyDrive/UNIT/model_checkpoint.h5\"\n",
        "    csv_path = \"/content/drive/MyDrive/UNIT/data_checkpoint.csv\"\n",
        "\n",
        "    \"\"\" Dataset : 60/20/20 \"\"\"\n",
        "    dataset_path = \"/content/drive/MyDrive\"\n",
        "    (train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_data(dataset_path)\n",
        "\n",
        "    print(f\"Train: {len(train_x)} - {len(train_y)}\")\n",
        "    print(f\"Valid: {len(valid_x)} - {len(valid_y)}\")\n",
        "    print(f\"Test: {len(test_x)} - {len(test_y)}\")\n",
        "\n",
        "    train_dataset = tf_dataset(train_x, train_y, batch_size)\n",
        "    valid_dataset = tf_dataset(valid_x, valid_y, batch_size)\n",
        "\n",
        "    train_steps = len(train_x)//batch_size\n",
        "    valid_steps = len(valid_x)//batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkKVgx_I1WQx"
      },
      "outputs": [],
      "source": [
        "    callbacks = [\n",
        "        ModelCheckpoint(model_path, verbose=1, save_best_only=True),\n",
        "        #ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-7, verbose=1),\n",
        "        CSVLogger(csv_path),\n",
        "        TensorBoard(),\n",
        "        EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pj9wz7BJB16p"
      },
      "outputs": [],
      "source": [
        "model.load_weights('/content/drive/MyDrive/files/model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkonPNowuJ6h",
        "outputId": "1488c03d-6f0b-4741-c17c-50dd39353c58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.0781 - recall: 0.9121 - precision: 0.9380 \n",
            "Epoch 1: val_loss improved from inf to 0.23392, saving model to /content/drive/MyDrive/UNIT/model_checkpoint.h5\n",
            "78/78 [==============================] - 5627s 73s/step - loss: 0.0781 - recall: 0.9121 - precision: 0.9380 - val_loss: 0.2339 - val_recall: 0.8483 - val_precision: 0.8787\n",
            "Epoch 2/20\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.0780 - recall: 0.9109 - precision: 0.9367 \n",
            "Epoch 2: val_loss did not improve from 0.23392\n",
            "78/78 [==============================] - 3713s 48s/step - loss: 0.0780 - recall: 0.9109 - precision: 0.9367 - val_loss: 0.3717 - val_recall: 0.9388 - val_precision: 0.7445\n",
            "Epoch 3/20\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.0701 - recall: 0.9201 - precision: 0.9436\n",
            "Epoch 3: val_loss improved from 0.23392 to 0.19173, saving model to /content/drive/MyDrive/UNIT/model_checkpoint.h5\n",
            "78/78 [==============================] - 1002s 13s/step - loss: 0.0701 - recall: 0.9201 - precision: 0.9436 - val_loss: 0.1917 - val_recall: 0.8716 - val_precision: 0.8733\n",
            "Epoch 4/20\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.0645 - recall: 0.9257 - precision: 0.9479 "
          ]
        }
      ],
      "source": [
        "    #50-54\n",
        "model.fit(\n",
        "        train_dataset,\n",
        "        epochs=20,\n",
        "        validation_data=valid_dataset,\n",
        "        callbacks=callbacks\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dusU3IeeSniI",
        "outputId": "3185d91b-cb4e-47fc-8bf1-6bc3d30e3f69"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "model.save('/content/drive/MyDrive/UNET/my_model_20oct.h5')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.metrics import Accuracy,Recall,Precision\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "metrics = [dice_coef, iou, Recall(), Precision()]\n",
        "\n"
      ],
      "metadata": {
        "id": "6vkVwfKbDfsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb5M2yyj9kPk"
      },
      "outputs": [],
      "source": [
        "new_model = tf.keras.models.load_model('/content/drive/MyDrive/UNIT/model_checkpoint.h5', custom_objects={'dice_coef': dice_coef, 'iou':iou})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hg8iMMEtSkd-",
        "outputId": "679a8827-7a04-4751-e014-f0f151547f46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.0628 - recall: 0.9303 - precision: 0.9467 \n",
            "Epoch 1: val_loss improved from inf to 0.22538, saving model to /content/drive/MyDrive/UNIT/model_checkpoint.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r78/78 [==============================] - 2145s 24s/step - loss: 0.0628 - recall: 0.9303 - precision: 0.9467 - val_loss: 0.2254 - val_recall: 0.8820 - val_precision: 0.8545\n",
            "Epoch 2/20\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.0792 - recall: 0.9069 - precision: 0.9401 \n",
            "Epoch 2: val_loss did not improve from 0.22538\n",
            "78/78 [==============================] - 1908s 24s/step - loss: 0.0792 - recall: 0.9069 - precision: 0.9401 - val_loss: 1.1661 - val_recall: 0.9581 - val_precision: 0.5547\n",
            "Epoch 3/20\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.1027 - recall: 0.8795 - precision: 0.9216\n",
            "Epoch 3: val_loss did not improve from 0.22538\n",
            "78/78 [==============================] - 809s 10s/step - loss: 0.1027 - recall: 0.8795 - precision: 0.9216 - val_loss: 0.4098 - val_recall: 0.9250 - val_precision: 0.7651\n",
            "Epoch 4/20\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.0757 - recall: 0.9175 - precision: 0.9336\n",
            "Epoch 4: val_loss improved from 0.22538 to 0.22085, saving model to /content/drive/MyDrive/UNIT/model_checkpoint.h5\n",
            "78/78 [==============================] - 595s 8s/step - loss: 0.0757 - recall: 0.9175 - precision: 0.9336 - val_loss: 0.2209 - val_recall: 0.8968 - val_precision: 0.8493\n",
            "Epoch 5/20\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.0633 - recall: 0.9285 - precision: 0.9464 \n",
            "Epoch 5: val_loss improved from 0.22085 to 0.17883, saving model to /content/drive/MyDrive/UNIT/model_checkpoint.h5\n",
            "78/78 [==============================] - 1830s 23s/step - loss: 0.0633 - recall: 0.9285 - precision: 0.9464 - val_loss: 0.1788 - val_recall: 0.8543 - val_precision: 0.9109\n",
            "Epoch 6/20\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.0563 - recall: 0.9362 - precision: 0.9525 \n",
            "Epoch 6: val_loss did not improve from 0.17883\n",
            "78/78 [==============================] - 1722s 22s/step - loss: 0.0563 - recall: 0.9362 - precision: 0.9525 - val_loss: 0.1839 - val_recall: 0.8588 - val_precision: 0.8996\n",
            "Epoch 7/20\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.0533 - recall: 0.9394 - precision: 0.9548\n",
            "Epoch 7: val_loss did not improve from 0.17883\n",
            "78/78 [==============================] - 559s 7s/step - loss: 0.0533 - recall: 0.9394 - precision: 0.9548 - val_loss: 0.1809 - val_recall: 0.8668 - val_precision: 0.8954\n",
            "Epoch 8/20\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.0529 - recall: 0.9406 - precision: 0.9547\n",
            "Epoch 8: val_loss did not improve from 0.17883\n",
            "78/78 [==============================] - 560s 7s/step - loss: 0.0529 - recall: 0.9406 - precision: 0.9547 - val_loss: 0.1975 - val_recall: 0.8429 - val_precision: 0.9055\n",
            "Epoch 9/20\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.0529 - recall: 0.9402 - precision: 0.9547\n",
            "Epoch 9: val_loss did not improve from 0.17883\n",
            "78/78 [==============================] - 557s 7s/step - loss: 0.0529 - recall: 0.9402 - precision: 0.9547 - val_loss: 0.1814 - val_recall: 0.8518 - val_precision: 0.9041\n",
            "Epoch 10/20\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.0516 - recall: 0.9430 - precision: 0.9544\n",
            "Epoch 10: val_loss did not improve from 0.17883\n",
            "78/78 [==============================] - 560s 7s/step - loss: 0.0516 - recall: 0.9430 - precision: 0.9544 - val_loss: 0.1951 - val_recall: 0.8587 - val_precision: 0.8941\n",
            "Epoch 11/20\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.0492 - recall: 0.9433 - precision: 0.9581\n",
            "Epoch 11: val_loss did not improve from 0.17883\n",
            "78/78 [==============================] - 559s 7s/step - loss: 0.0492 - recall: 0.9433 - precision: 0.9581 - val_loss: 0.2142 - val_recall: 0.8674 - val_precision: 0.8765\n",
            "Epoch 12/20\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.0491 - recall: 0.9454 - precision: 0.9569\n",
            "Epoch 12: val_loss did not improve from 0.17883\n",
            "78/78 [==============================] - 557s 7s/step - loss: 0.0491 - recall: 0.9454 - precision: 0.9569 - val_loss: 0.2012 - val_recall: 0.8355 - val_precision: 0.9230\n",
            "Epoch 13/20\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.0513 - recall: 0.9423 - precision: 0.9550\n",
            "Epoch 13: val_loss did not improve from 0.17883\n",
            "78/78 [==============================] - 555s 7s/step - loss: 0.0513 - recall: 0.9423 - precision: 0.9550 - val_loss: 0.2082 - val_recall: 0.8606 - val_precision: 0.8978\n",
            "Epoch 14/20\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.0488 - recall: 0.9445 - precision: 0.9578\n",
            "Epoch 14: val_loss did not improve from 0.17883\n",
            "78/78 [==============================] - 556s 7s/step - loss: 0.0488 - recall: 0.9445 - precision: 0.9578 - val_loss: 0.2057 - val_recall: 0.8640 - val_precision: 0.8904\n",
            "Epoch 15/20\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.0482 - recall: 0.9457 - precision: 0.9576\n",
            "Epoch 15: val_loss did not improve from 0.17883\n",
            "78/78 [==============================] - 562s 7s/step - loss: 0.0482 - recall: 0.9457 - precision: 0.9576 - val_loss: 0.2042 - val_recall: 0.8460 - val_precision: 0.9140\n",
            "Epoch 16/20\n",
            "39/78 [==============>...............] - ETA: 3:17 - loss: 0.0462 - recall: 0.9486 - precision: 0.9583"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnknownError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-74a5523eac41>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#54-56\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m new_model.fit(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnknownError\u001b[0m: Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\n2 root error(s) found.\n  (0) UNKNOWN:  error: OpenCV(4.8.0) /io/opencv/modules/imgproc/src/resize.cpp:4062: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/tmp/__autograph_generated_fileogwdbaav.py\", line 17, in _parse\n    y = ag__.converted_call(ag__.ld(read_mask), (ag__.ld(y),), None, fscope_1)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\", line 335, in converted_call\n    return _call_unconverted(f, args, kwargs, options, False)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\", line 460, in _call_unconverted\n    return f(*args)\n\n  File \"<ipython-input-15-955421c57c1e>\", line 12, in read_mask\n    x = cv2.resize(x, (W, H))\n\ncv2.error: OpenCV(4.8.0) /io/opencv/modules/imgproc/src/resize.cpp:4062: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[gradient_tape/binary_crossentropy/logistic_loss/mul/Shape_1/_10]]\n  (1) UNKNOWN:  error: OpenCV(4.8.0) /io/opencv/modules/imgproc/src/resize.cpp:4062: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/tmp/__autograph_generated_fileogwdbaav.py\", line 17, in _parse\n    y = ag__.converted_call(ag__.ld(read_mask), (ag__.ld(y),), None, fscope_1)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\", line 335, in converted_call\n    return _call_unconverted(f, args, kwargs, options, False)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\", line 460, in _call_unconverted\n    return f(*args)\n\n  File \"<ipython-input-15-955421c57c1e>\", line 12, in read_mask\n    x = cv2.resize(x, (W, H))\n\ncv2.error: OpenCV(4.8.0) /io/opencv/modules/imgproc/src/resize.cpp:4062: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_13153]"
          ]
        }
      ],
      "source": [
        "    #55-70\n",
        "new_model.fit(\n",
        "        train_dataset,\n",
        "        epochs=20,\n",
        "        validation_data=valid_dataset,\n",
        "        callbacks=callbacks\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_model.compile(loss=\"binary_crossentropy\", optimizer=Adam(0.0001), metrics=metrics)"
      ],
      "metadata": {
        "id": "Nh8XW-GqURe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvdBBIC3Wcvj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0761c818-108e-4768-ea4a-321d84669730"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.0499 - dice_coef: 0.9263 - iou: 0.8629 - recall_3: 0.9448 - precision_3: 0.9561\n",
            "Epoch 1: val_loss did not improve from 0.17883\n",
            "78/78 [==============================] - 746s 8s/step - loss: 0.0499 - dice_coef: 0.9263 - iou: 0.8629 - recall_3: 0.9448 - precision_3: 0.9561 - val_loss: 0.2173 - val_dice_coef: 0.8669 - val_iou: 0.7673 - val_recall_3: 0.8615 - val_precision_3: 0.9007\n",
            "Epoch 2/20\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.0446 - dice_coef: 0.9335 - iou: 0.8755 - recall_3: 0.9506 - precision_3: 0.9601\n",
            "Epoch 2: val_loss did not improve from 0.17883\n",
            "78/78 [==============================] - 690s 9s/step - loss: 0.0446 - dice_coef: 0.9335 - iou: 0.8755 - recall_3: 0.9506 - precision_3: 0.9601 - val_loss: 0.2145 - val_dice_coef: 0.8564 - val_iou: 0.7510 - val_recall_3: 0.8343 - val_precision_3: 0.9126\n",
            "Epoch 3/20\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.0429 - dice_coef: 0.9360 - iou: 0.8798 - recall_3: 0.9516 - precision_3: 0.9621\n",
            "Epoch 3: val_loss did not improve from 0.17883\n",
            "78/78 [==============================] - 692s 9s/step - loss: 0.0429 - dice_coef: 0.9360 - iou: 0.8798 - recall_3: 0.9516 - precision_3: 0.9621 - val_loss: 0.2177 - val_dice_coef: 0.8605 - val_iou: 0.7573 - val_recall_3: 0.8523 - val_precision_3: 0.8989\n",
            "Epoch 4/20\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.0449 - dice_coef: 0.9332 - iou: 0.8750 - recall_3: 0.9503 - precision_3: 0.9602\n",
            "Epoch 4: val_loss did not improve from 0.17883\n",
            "78/78 [==============================] - 690s 9s/step - loss: 0.0449 - dice_coef: 0.9332 - iou: 0.8750 - recall_3: 0.9503 - precision_3: 0.9602 - val_loss: 0.2208 - val_dice_coef: 0.8570 - val_iou: 0.7520 - val_recall_3: 0.8788 - val_precision_3: 0.8668\n",
            "Epoch 5/20\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.0410 - dice_coef: 0.9388 - iou: 0.8847 - recall_3: 0.9540 - precision_3: 0.9639\n",
            "Epoch 5: val_loss did not improve from 0.17883\n",
            "78/78 [==============================] - 719s 9s/step - loss: 0.0410 - dice_coef: 0.9388 - iou: 0.8847 - recall_3: 0.9540 - precision_3: 0.9639 - val_loss: 0.2099 - val_dice_coef: 0.8589 - val_iou: 0.7553 - val_recall_3: 0.8491 - val_precision_3: 0.9003\n",
            "Epoch 6/20\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.0415 - dice_coef: 0.9379 - iou: 0.8832 - recall_3: 0.9538 - precision_3: 0.9627\n",
            "Epoch 6: val_loss did not improve from 0.17883\n",
            "78/78 [==============================] - 703s 9s/step - loss: 0.0415 - dice_coef: 0.9379 - iou: 0.8832 - recall_3: 0.9538 - precision_3: 0.9627 - val_loss: 0.2563 - val_dice_coef: 0.8585 - val_iou: 0.7546 - val_recall_3: 0.8820 - val_precision_3: 0.8623\n",
            "Epoch 7/20\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.0418 - dice_coef: 0.9380 - iou: 0.8835 - recall_3: 0.9534 - precision_3: 0.9629\n",
            "Epoch 7: val_loss did not improve from 0.17883\n",
            "78/78 [==============================] - 702s 9s/step - loss: 0.0418 - dice_coef: 0.9380 - iou: 0.8835 - recall_3: 0.9534 - precision_3: 0.9629 - val_loss: 0.2201 - val_dice_coef: 0.8588 - val_iou: 0.7557 - val_recall_3: 0.8370 - val_precision_3: 0.9125\n",
            "Epoch 8/20\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.0447 - dice_coef: 0.9338 - iou: 0.8760 - recall_3: 0.9504 - precision_3: 0.9603\n",
            "Epoch 8: val_loss did not improve from 0.17883\n",
            "78/78 [==============================] - 721s 9s/step - loss: 0.0447 - dice_coef: 0.9338 - iou: 0.8760 - recall_3: 0.9504 - precision_3: 0.9603 - val_loss: 0.2419 - val_dice_coef: 0.8638 - val_iou: 0.7629 - val_recall_3: 0.8675 - val_precision_3: 0.8890\n",
            "Epoch 9/20\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.0450 - dice_coef: 0.9332 - iou: 0.8750 - recall_3: 0.9498 - precision_3: 0.9598\n",
            "Epoch 9: val_loss did not improve from 0.17883\n",
            "78/78 [==============================] - 747s 10s/step - loss: 0.0450 - dice_coef: 0.9332 - iou: 0.8750 - recall_3: 0.9498 - precision_3: 0.9598 - val_loss: 0.2270 - val_dice_coef: 0.8611 - val_iou: 0.7585 - val_recall_3: 0.8546 - val_precision_3: 0.8964\n",
            "Epoch 10/20\n",
            "52/78 [===================>..........] - ETA: 2:56 - loss: 0.0414 - dice_coef: 0.9386 - iou: 0.8845 - recall_3: 0.9565 - precision_3: 0.9610"
          ]
        }
      ],
      "source": [
        "#70-80\n",
        "new_model.fit(\n",
        "        train_dataset,\n",
        "        epochs=20,\n",
        "        validation_data=valid_dataset,\n",
        "        callbacks=callbacks\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#86-88\n",
        "new_model.fit(\n",
        "        train_dataset,\n",
        "        epochs=20,\n",
        "        validation_data=valid_dataset,\n",
        "        callbacks=callbacks\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txdfm5R8CyMI",
        "outputId": "8eb0fff5-ea59-4e16-8032-0dd18e01ca60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.0522 - dice_coef: 0.9219 - iou: 0.8554 - recall_1: 0.9413 - precision_1: 0.9544 \n",
            "Epoch 1: val_loss improved from inf to 0.17627, saving model to /content/drive/MyDrive/UNIT/model_checkpoint.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r78/78 [==============================] - 2548s 31s/step - loss: 0.0522 - dice_coef: 0.9219 - iou: 0.8554 - recall_1: 0.9413 - precision_1: 0.9544 - val_loss: 0.1763 - val_dice_coef: 0.8633 - val_iou: 0.7613 - val_recall_1: 0.8680 - val_precision_1: 0.9008\n",
            "Epoch 2/20\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.0524 - dice_coef: 0.9215 - iou: 0.8547 - recall_1: 0.9416 - precision_1: 0.9527 \n",
            "Epoch 2: val_loss did not improve from 0.17627\n",
            "78/78 [==============================] - 3105s 40s/step - loss: 0.0524 - dice_coef: 0.9215 - iou: 0.8547 - recall_1: 0.9416 - precision_1: 0.9527 - val_loss: 0.2198 - val_dice_coef: 0.8626 - val_iou: 0.7605 - val_recall_1: 0.8801 - val_precision_1: 0.8798\n",
            "Epoch 3/20\n",
            " 4/78 [>.............................] - ETA: 30:09 - loss: 0.0628 - dice_coef: 0.9176 - iou: 0.8479 - recall_1: 0.9022 - precision_1: 0.9742"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#91-100\n",
        "new_model.fit(\n",
        "        train_dataset,\n",
        "        epochs=20,\n",
        "        validation_data=valid_dataset,\n",
        "        callbacks=callbacks\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmFVC3HjRuun",
        "outputId": "b266c296-acd6-4910-9c88-ba55a6d52e7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.0518 - dice_coef: 0.9231 - iou: 0.8575 - recall_1: 0.9432 - precision_1: 0.9531 \n",
            "Epoch 1: val_loss improved from inf to 0.25114, saving model to /content/drive/MyDrive/UNIT/model_checkpoint.h5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "78/78 [==============================] - 2657s 25s/step - loss: 0.0518 - dice_coef: 0.9231 - iou: 0.8575 - recall_1: 0.9432 - precision_1: 0.9531 - val_loss: 0.2511 - val_dice_coef: 0.8597 - val_iou: 0.7558 - val_recall_1: 0.8678 - val_precision_1: 0.8830\n",
            "Epoch 2/20\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.0494 - dice_coef: 0.9267 - iou: 0.8636 - recall_1: 0.9446 - precision_1: 0.9561 \n",
            "Epoch 2: val_loss improved from 0.25114 to 0.20973, saving model to /content/drive/MyDrive/UNIT/model_checkpoint.h5\n",
            "78/78 [==============================] - 2211s 28s/step - loss: 0.0494 - dice_coef: 0.9267 - iou: 0.8636 - recall_1: 0.9446 - precision_1: 0.9561 - val_loss: 0.2097 - val_dice_coef: 0.8601 - val_iou: 0.7568 - val_recall_1: 0.8195 - val_precision_1: 0.9333\n",
            "Epoch 3/20\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.0466 - dice_coef: 0.9302 - iou: 0.8696 - recall_1: 0.9481 - precision_1: 0.9579 \n",
            "Epoch 3: val_loss did not improve from 0.20973\n",
            "78/78 [==============================] - 2045s 26s/step - loss: 0.0466 - dice_coef: 0.9302 - iou: 0.8696 - recall_1: 0.9481 - precision_1: 0.9579 - val_loss: 0.2275 - val_dice_coef: 0.8634 - val_iou: 0.7619 - val_recall_1: 0.8741 - val_precision_1: 0.8821\n",
            "Epoch 4/20\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.0455 - dice_coef: 0.9322 - iou: 0.8731 - recall_1: 0.9484 - precision_1: 0.9596\n",
            "Epoch 4: val_loss improved from 0.20973 to 0.20105, saving model to /content/drive/MyDrive/UNIT/model_checkpoint.h5\n",
            "78/78 [==============================] - 757s 10s/step - loss: 0.0455 - dice_coef: 0.9322 - iou: 0.8731 - recall_1: 0.9484 - precision_1: 0.9596 - val_loss: 0.2010 - val_dice_coef: 0.8626 - val_iou: 0.7605 - val_recall_1: 0.8610 - val_precision_1: 0.8965\n",
            "Epoch 5/20\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.0447 - dice_coef: 0.9334 - iou: 0.8753 - recall_1: 0.9503 - precision_1: 0.9600 \n",
            "Epoch 5: val_loss did not improve from 0.20105\n",
            "78/78 [==============================] - 2233s 29s/step - loss: 0.0447 - dice_coef: 0.9334 - iou: 0.8753 - recall_1: 0.9503 - precision_1: 0.9600 - val_loss: 0.2369 - val_dice_coef: 0.8599 - val_iou: 0.7569 - val_recall_1: 0.8405 - val_precision_1: 0.9071\n",
            "Epoch 6/20\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.0469 - dice_coef: 0.9312 - iou: 0.8715 - recall_1: 0.9469 - precision_1: 0.9591\n",
            "Epoch 6: val_loss did not improve from 0.20105\n",
            "78/78 [==============================] - 717s 9s/step - loss: 0.0469 - dice_coef: 0.9312 - iou: 0.8715 - recall_1: 0.9469 - precision_1: 0.9591 - val_loss: 0.2616 - val_dice_coef: 0.8603 - val_iou: 0.7580 - val_recall_1: 0.8910 - val_precision_1: 0.8599\n",
            "Epoch 7/20\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.0481 - dice_coef: 0.9289 - iou: 0.8676 - recall_1: 0.9457 - precision_1: 0.9576\n",
            "Epoch 7: val_loss did not improve from 0.20105\n",
            "78/78 [==============================] - 665s 8s/step - loss: 0.0481 - dice_coef: 0.9289 - iou: 0.8676 - recall_1: 0.9457 - precision_1: 0.9576 - val_loss: 0.2472 - val_dice_coef: 0.8705 - val_iou: 0.7725 - val_recall_1: 0.8775 - val_precision_1: 0.8897\n",
            "Epoch 8/20\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.0519 - dice_coef: 0.9239 - iou: 0.8589 - recall_1: 0.9417 - precision_1: 0.9545\n",
            "Epoch 8: val_loss did not improve from 0.20105\n",
            "78/78 [==============================] - 664s 8s/step - loss: 0.0519 - dice_coef: 0.9239 - iou: 0.8589 - recall_1: 0.9417 - precision_1: 0.9545 - val_loss: 0.2922 - val_dice_coef: 0.8552 - val_iou: 0.7496 - val_recall_1: 0.8599 - val_precision_1: 0.8776\n",
            "Epoch 9/20\n",
            " 9/78 [==>...........................] - ETA: 6:19 - loss: 0.0588 - dice_coef: 0.9106 - iou: 0.8363 - recall_1: 0.9311 - precision_1: 0.9525"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MnXT_bc2jk84"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}